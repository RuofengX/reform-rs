{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 依赖声明\n",
    "\n",
    "## 环境搭建\n",
    "### 基本环境\n",
    "\n",
    "电脑须安装有anaconda或其他提供conda环境的框架。  \n",
    "之后运行以下代码安装依赖\n",
    "\n",
    "```shell\n",
    "conda install ipython ipykernel ipywidgets pyarrow pandas -y\n",
    "```\n",
    "注意：必须用conda安装兼容的pyarrow和pandas，两者版本容易冲突\n",
    "\n",
    "### 必备工具\n",
    "\n",
    "pip安装hanlp以及持久缓存  \n",
    "```\n",
    "pip install hanlp[full] cachier\n",
    "```\n",
    "\n",
    "### 开发工具\n",
    "\n",
    "1. 安装nbstrpout工具，在每次提交时删除此文件中的敏感数据\n",
    "    ```\n",
    "    pip install nbstripout\n",
    "    nbstripout --install\n",
    "    nbstripout --status\n",
    "    ```\n",
    "2. 安装memory_profiler，监控内存占用\n",
    "    ```\n",
    "    pip install memory_profiler\n",
    "    ```\n",
    "    用法：%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "from pprint import pprint as print\n",
    "from functools import cache\n",
    "\n",
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "import hanlp\n",
    "from hanlp.components.mtl.tasks.ner.tag_ner import TaggingNamedEntityRecognition\n",
    "from hanlp.components.tokenizers.transformer import TransformerTaggingTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from cachier import cachier\n",
    "\n",
    "cachier = cachier(cache_dir='.py_cache')\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. NLP配置\n",
    " ~~大炮打蚊子的第一步是把大炮架起来~~\n",
    "\n",
    "这里使用HanLP作为辅助处理工具，文档可以参考：[Github文档](https://github.com/hankcs/HanLP/tree/doc-zh)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载世界最大中文语料库\n",
    "_nlp_model = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH)  # type: ignore\n",
    "# ner: TaggingNamedEntityRecognition = _nlp_model['ner/msra']\n",
    "# ner.dict_whitelist = {'全账户': \"\"}\n",
    "\n",
    "# 配置精分固定短语\n",
    "_nlp_model['tok/fine'].dict_force = {\"全账户\"}\n",
    "\n",
    "\n",
    "# ner的意思是`命名实体识别`，可有效识别出人名、组织名等\n",
    "@cachier\n",
    "def ner(value) -> list[tuple[str, str, int, int]]:\n",
    "    result = _nlp_model(\n",
    "        value,\n",
    "        tasks=\"ner/msra\",\n",
    "    )[\"ner/msra\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "def ner_first_name(value: str) -> Optional[str]:\n",
    "    for content, type, _start, _end in ner(value):\n",
    "        if type == \"PERSON\":\n",
    "            return content\n",
    "\n",
    "\n",
    "def ner_path_rev(path: str) -> Optional[str]:\n",
    "    path = str(path)\n",
    "    # 将value按照分割符号切割，避免错误识别\n",
    "    path_part = path.split(os.path.sep)\n",
    "\n",
    "    # 从后往前迭代，更后的文件夹更有可能是文件的名字\n",
    "    path_part.reverse()\n",
    "    for i in path_part:\n",
    "        result = ner_first_name(i)\n",
    "        if result is None:\n",
    "            pass\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = _nlp_model(\n",
    "    [\"全账户分析自动化脚本，作者韦若枫，mailto://i@ruofengx.cn\"],\n",
    "    tasks=\"ner/msra\",\n",
    ")\n",
    "\n",
    "result.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. 常用函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_not_none(*values):\n",
    "    for v in values:\n",
    "        if v is None:\n",
    "            continue\n",
    "        else:\n",
    "            return v\n",
    "    return None\n",
    "def collect_not_none(*values):\n",
    "    ret = list()\n",
    "    for v in values:\n",
    "        if v is not None:\n",
    "            ret.append(v)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 读取数据\n",
    "\n",
    "读取由rust程序生成的parquet文件\n",
    "\n",
    "**数据列说明**\n",
    "\n",
    "- **_config_name**  \n",
    "  \n",
    "  `str`\n",
    "  \n",
    "  前期处理的配置名称\n",
    "\n",
    "- **_datetime**  \n",
    "  \n",
    "  `datetime.datetime`\n",
    "  \n",
    "  转账的日期，实际精度到秒。\n",
    "  \n",
    "  存在部分银行整点执行的自动划转，实际精度为日，精度自动扩大至当日零时零分零秒。  \n",
    "\n",
    "- **_amount**  \n",
    "\n",
    "  `decimal.Decimal`\n",
    "  \n",
    "  保留两位小数，整数部分精度为36位\n",
    "\n",
    "- **_from_id**  \n",
    "\n",
    "  `str`\n",
    "  \n",
    "  转账发出人在该平台的唯一ID\n",
    "\n",
    "  应为非空字符串，但实际上存在空字符串的可能，需进一步研究TODO\n",
    "\n",
    "- **_from_bank_id**  \n",
    "\n",
    "  `Optional[str]`\n",
    "  \n",
    "  转账发出人的银行卡号，如账单未定义则为None\n",
    "\n",
    "  存在部分平台账单在此列数据填入非银行卡号，需要正则清洗\n",
    "  为非空字符串，但实际上存在空字符串的可能，建议drop\n",
    "\n",
    "  也有的银行账单会填入存折号，清洗时注意\n",
    "\n",
    "- **_from_name**  \n",
    "\n",
    "  `Optional[str]`\n",
    "  \n",
    "  转账发出人的名字，也可以是商家名字\n",
    "\n",
    "- **_to_id**\n",
    "\n",
    "  `str`\n",
    "  \n",
    "  转账接收在该平台的唯一ID\n",
    "\n",
    "  应为非空字符串，但实际上存在空字符串的可能，需进一步研究TODO\n",
    "\n",
    "- **_to_bank_id**\n",
    "\n",
    "  `Optional[str]`类型，转账接收人的银行卡号，如账单未定义则为None\n",
    "\n",
    "  存在部分平台账单在此列数据填入非银行卡号，需要正则清洗\n",
    "  为非空字符串，但实际上存在空字符串的可能，建议drop\n",
    "\n",
    "  也有的银行账单会填入存折号，清洗时注意\n",
    "\n",
    "\n",
    "- **_to_name**\n",
    "\n",
    "  `Optional[str]`类型，转账发出人的名字，也可以是商家名字\n",
    "\n",
    "- **_prime_id**\n",
    "\n",
    "  `Optional[str]`类型，查询主体的ID，在个别账单中缺失，需要通过路径、文件名补齐\n",
    "\n",
    "- **_file_path**\n",
    "\n",
    "  `str`\n",
    "\n",
    "  文件路径，可通过NLP分析出账单主体，并和**_from_name**或**_to_name**对比，确认查询主体\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"./tmp/batch.parquet\")\n",
    "print(df.columns.tolist())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. 检查数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. 检查双侧ID均空的行\n",
    "\n",
    "输出结果应为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['_from_id'].isnull() & df['_to_id'].isnull()].__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. 检查每一文件的条数\n",
    "\n",
    "部分很短小的账单会影响后续按文件聚类提取信息的置信度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df[[\"_file_path\"]].groupby(\"_file_path\").value_counts(), on=\"_file_path\").rename(columns={\"count\":\"record_count\"})\n",
    "for i in [10,5,1]:\n",
    "    count = df[df[\"record_count\"] <= i].drop_duplicates(\"_file_path\").__len__()\n",
    "    print(f\"单个账单条数小于等于{i}条的有： {count}条\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. 检查主体缺失项情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"_prime_id\"].isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"_prime_id\"].isnull(), \"_file_path\"].drop_duplicates().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 定义数据模型(暂时无用)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目前没啥用\n",
    "@dataclass\n",
    "class Entity:\n",
    "    id: str\n",
    "    bank_id: Optional[str]\n",
    "    name: Optional[str]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Record:\n",
    "    config: str\n",
    "    time: datetime\n",
    "    amount: Decimal\n",
    "    from_: Entity\n",
    "    to: Entity\n",
    "    path: str\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return (\n",
    "            self.time.__hash__()\n",
    "            + self.amount.__hash__()\n",
    "            + self.from_.id.__hash__()\n",
    "            + self.to.id.__hash__()\n",
    "        )\n",
    "\n",
    "\n",
    "def row_to_record(row: pd.Series) -> Record:\n",
    "    from_entity = Entity(\n",
    "        id=row[\"_from_id\"], bank_id=row[\"_from_bank_id\"], name=row[\"_from_name\"]  # type: ignore\n",
    "    )\n",
    "    to_entity = Entity(\n",
    "        id=row[\"_to_id\"], bank_id=row[\"_to_bank_id\"], name=row[\"_to_name\"]  # type: ignore\n",
    "    )\n",
    "    return Record(\n",
    "        config=row[\"_config_name\"],  # type: ignore\n",
    "        time=row[\"_datetime\"],  # type: ignore\n",
    "        amount=row[\"_amount\"],  # type: ignore\n",
    "        from_=from_entity,\n",
    "        to=to_entity,\n",
    "        path=row[\"_file_path\"],  # type: ignore\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算数据模型，并存储到`ent`列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"ent\"] = df.progress_apply(row_to_record, axis=1)  # type:ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 数据清洗\n",
    "\n",
    "1. 将该文件查询对象（查询的银行卡、财付通账号等等，总是_from_id或_to_id中的一个）关联至该主体，对于一个文件，查询对象总是至少占一半。这个规律不一定适用步骤2。\n",
    "2. 确定一个文件的主体姓名，首先通过NLP分析文件路径_file_path确定人名，其次可以通过出现频率大于一半的 _*_name 字段值即为主体人名\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 确定调证主体名字\n",
    "\n",
    "`prime_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. 过滤字符串"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先把\"(个人)\"从字符串中删除  \n",
    "这个无效标记在国反三方账单中非常常见\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OP_COL = [\"_from_name\", \"_to_name\"]\n",
    "for col in OP_COL:\n",
    "    df[col] = df[col].str.replace(\"(个人)\", \"\", regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. 同文件级别最常出现的name\n",
    "\n",
    "`file_common_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.1. 获取全部names\n",
    "\n",
    "的from_names和to_names，并合并为names表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_names = df[[\"_file_path\", \"_from_name\"]].copy()\n",
    "to_names = df[[\"_file_path\", \"_to_name\"]].copy()\n",
    "\n",
    "from_names.rename(columns={\"_from_name\": \"file_common_name\"}, inplace=True)\n",
    "to_names.rename(columns={\"_to_name\": \"file_common_name\"}, inplace=True)\n",
    "\n",
    "names = pd.concat([from_names, to_names])\n",
    "# names.set_index(\"_file_path\", inplace=True)\n",
    "\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.2. 统计name频数\n",
    "\n",
    "同文件的name出现频数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_names_count = names[[\"_file_path\", \"file_common_name\"]].value_counts().reset_index()\n",
    "_names_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.3. 递归过滤最常出现的name\n",
    "\n",
    "迭代file_common_name，利用ner函数排除机构组织名字，直至频数最大的file_common_name的为人名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 递归验证频率最高的name是人名，不是人名则从names_count删除，继续下一个\n",
    "# 这么做可以避免全量计算ner\n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    most_name_for_file = _names_count[[\"_file_path\", \"file_common_name\"]].loc[\n",
    "        _names_count.groupby(\"_file_path\")[\"count\"].idxmax()\n",
    "    ]\n",
    "    most_name_for_file[\"ner_name\"] = most_name_for_file[\n",
    "        \"file_common_name\"\n",
    "    ].progress_apply(ner_first_name)\n",
    "    bad_names = most_name_for_file[most_name_for_file[\"ner_name\"].isnull()][\n",
    "        \"file_common_name\"\n",
    "    ].to_list()\n",
    "    if bad_names.__len__() > 0:\n",
    "        _names_count = _names_count[\n",
    "            _names_count[\"file_common_name\"].apply(lambda x: x not in bad_names)\n",
    "        ]\n",
    "        print(f\"第{i}次迭代{_names_count.__len__()}\")\n",
    "        print(_names_count.shape)\n",
    "    else:\n",
    "        break\n",
    "most_name_for_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2.4. 附加到df表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_name_for_file = most_name_for_file.set_index('_file_path').drop(columns=\"ner_name\") # 格式化表\n",
    "df = df.join(most_name_for_file, on=\"_file_path\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. 同文件夹最常出现的name\n",
    "\n",
    "`folder_common_name`\n",
    "\n",
    "file_common_name 会有部分为空，这种情况会出现在账单非常稀缺的情况，甚至干脆是空表。  \n",
    "这个时候可以用同样的办法研判同文件夹下最常出现的name作为依据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似file_common_name的步骤，不再赘述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取文件夹路径\n",
    "df[\"folder_path\"] = df[\"_file_path\"].apply(lambda x: os.path.dirname(x))\n",
    "# 取文件夹出现的名字表names\n",
    "from_names = df[[\"folder_path\", \"_from_name\"]].copy()\n",
    "to_names = df[[\"folder_path\", \"_to_name\"]].copy()\n",
    "from_names.rename(columns={\"_from_name\": \"folder_common_name\"}, inplace=True)\n",
    "to_names.rename(columns={\"_to_name\": \"folder_common_name\"}, inplace=True)\n",
    "names = pd.concat([from_names, to_names])\n",
    "names.set_index(\"folder_path\", inplace=True)\n",
    "\n",
    "\n",
    "# 对names计数得到most_name_for_folder\n",
    "_names_count = names.groupby([\"folder_path\", \"folder_common_name\"]).value_counts().reset_index()\n",
    "most_name_for_folder = _names_count[[\"folder_path\", \"folder_common_name\"]].loc[_names_count.groupby(\"folder_path\")[\"count\"].idxmax()]\n",
    "most_name_for_folder.set_index(\"folder_path\", inplace=True)\n",
    "\n",
    "# 迭代至全是人名\n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    most_name_for_folder = _names_count[[\"folder_path\", \"folder_common_name\"]].loc[\n",
    "        _names_count.groupby(\"folder_path\")[\"count\"].idxmax()\n",
    "    ]\n",
    "    most_name_for_folder[\"ner_name\"] = most_name_for_folder[\n",
    "        \"folder_common_name\"\n",
    "    ].progress_apply(ner_first_name)\n",
    "    bad_names = most_name_for_folder[most_name_for_folder[\"ner_name\"].isnull()][\n",
    "        \"folder_common_name\"\n",
    "    ].to_list()\n",
    "    if bad_names.__len__() > 0:\n",
    "        _names_count = _names_count[\n",
    "            _names_count[\"folder_common_name\"].apply(lambda x: x not in bad_names)\n",
    "        ]\n",
    "        print(f\"第{i}次迭代{_names_count.__len__()}\")\n",
    "        print(_names_count.shape)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 并入df\n",
    "most_name_for_folder = most_name_for_folder.set_index('folder_path').drop(columns=\"ner_name\") # 格式化表\n",
    "df = df.join(most_name_for_folder, on=\"folder_path\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. 从路径推理name\n",
    "\n",
    "`path_reason_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3.1. 举个例子\n",
    "\n",
    "可能不同的数据集的例子不一样，所以看看就行。  \n",
    "数据已作脱敏处理。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "example = df.iloc[277572]\n",
    "example\n",
    "```\n",
    "\n",
    "输出:\n",
    "```txt\n",
    "_config_name                                                    国反-三方\n",
    "_datetime                                         2022-03-10 17:41:32\n",
    "_amount                                                        193.99\n",
    "_from_id                                            134****055@qq.com\n",
    "_from_bank_id                                                    None\n",
    "_from_name                                                       None\n",
    "_to_id                                                             花呗\n",
    "_to_bank_id                                                      None\n",
    "_to_name                                                         None\n",
    "_file_path          ./tmp/batch/倪*全账户/134****055@qq.com(2021010100...\n",
    "...\n",
    "Name: 277572, dtype: object\n",
    "```\n",
    "\n",
    "这一行没有任何当事人的信息，只有一个_from_id，还是QQ邮箱地址，只有path中包含主体名字“倪*”。  \n",
    "如果此时`file_common_name`和`folder_common_name`均无效的话，就需要通过NLP对文件路径进行提取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用函数`ner_path`\n",
    "```python\n",
    "ner_path(example[\"_file_path\"])\n",
    "```\n",
    "得到想要的输出：\n",
    "\n",
    "'倪*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3.2. 正式开始\n",
    "\n",
    "首先，查看有多少文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"_file_path\"].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为涉及神经网络计算，速度会很慢，建议以下几点：\n",
    "* 对开头定义的ner加上持久化缓存cachier，对同一个路径碎片可以直接调用缓存\n",
    "* 安装`HanLP[full]`配套CUDA食用（未测试），或使用强力CPU（7950X3D大约50it/s）\n",
    "* 使用本地jupyter环境，远程环境带宽可以是瓶颈\n",
    "* 先去重，再计算，随后使用`df.join`方法插回去\n",
    "* 添加tqdm进度条，需要安装依赖，可参考[ipywidgets文档](https://ipywidgets.readthedocs.io/en/stable/user_install.html)，在开头应该安装过了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_reason_names = df.drop_duplicates(\"_file_path\")[[\"_file_path\"]]\n",
    "path_reason_names[\"path_reason_name\"]= path_reason_names[\"_file_path\"].progress_apply(ner_path_rev)\n",
    "path_reason_names.set_index(\"_file_path\", inplace=True)\n",
    "\n",
    "# 合并到df\n",
    "df = df.join(path_reason_names, on=\"_file_path\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. 合并主体\n",
    "\n",
    "`prime_name`\n",
    "\n",
    "目前有三个不同的主体类型，需要推断、清洗，得出最后的类型\n",
    "\n",
    "一般来说，`file_common_name`作为文件中最常出现的名字，有较高可信度。  \n",
    "但是部分账单这条不适用：\n",
    "\n",
    "比如（数据已脱敏）：\n",
    "```txt\n",
    "_config_name                                                      国反-三方\n",
    "_datetime                                           2024-03-25 15:51:58\n",
    "_amount                                                            6.00\n",
    "_from_id                                                    158******49\n",
    "_from_bank_id                                                      None\n",
    "_from_name                                                         None\n",
    "_to_id                                                t*******3@t**u.cn\n",
    "_to_bank_id                                                        None\n",
    "_to_name                                                   厦门*******有限公司\n",
    "_file_path            ./tmp/batch/8人全账户流水/刘*/158******49(20240101000...\n",
    "file_common_name                                                    NaN\n",
    "_folder_path                                     ./tmp/batch/8人全账户流水/刘*\n",
    "folder_common_name                                                   刘*\n",
    "path_reason_name                                                     刘*\n",
    "Name: 16554, dtype: object\n",
    "```\n",
    "这条账单中，`file_common_name`指向了一个公司，这里就需要将这类例子进行排除，  \n",
    "并先后以`path_reason_name`和`folder_common_name`取代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当`path_reason_name`为空时,此时说明文件夹制作者没有对文件进行分类，而是直接混杂放置  \n",
    "这个时候`folder_common_name`也不可信，直接选用`file_common_name`作为主体名字  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_row = df[df[\"path_reason_name\"].isnull()].index\n",
    "df.loc[filter_row, \"prime_name\"] = df.loc[filter_row, \"file_common_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "路径已成功解析名字的可以直接作为主体名字  \n",
    "TODO: 可以额外手动验证下冲突项中是否有错误的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_row = df[df[\"path_reason_name\"].notnull()].index\n",
    "df.loc[filter_row, \"prime_name\"] = df.loc[filter_row, \"path_reason_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还会剩余一些“三无”记录  \n",
    "只能通过ID进行关联"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未研判主体记录的数量\n",
    "df[df[\"prime_name\"].isna()].__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过以上步骤发现，`folder_common_name`似乎没啥用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. 补全查询主体ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. 文件内搜索\n",
    "使用缺失文件内计数最多的ID作为主体ID补全，仅适用于文件记录条数>=2、且双边ID均不缺失的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先筛选出需要补全的记录们"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选prime_id为空的行\n",
    "id_to_fill = df[df[\"_prime_id\"].isnull()].drop(columns=\"_prime_id\")\n",
    "id_to_fill.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后汇总统计同文件内的ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.concat(\n",
    "    [\n",
    "        id_to_fill.rename(columns={\"_from_id\": \"prime_id\"}),\n",
    "        id_to_fill.rename(columns={\"_to_id\": \"_prime_id\"}),\n",
    "    ]\n",
    ")[[\"_file_path\", \"_prime_id\"]]\n",
    "ids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对id计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_count = ids.value_counts().reset_index()\n",
    "ids_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取频数最大的ID制作MostIdForFile表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_id_for_file = ids_count.loc[ids_count.groupby(\"_file_path\")[\"count\"].idxmax()]\n",
    "most_id_for_file = most_id_for_file.set_index(\"_file_path\").drop(columns=\"count\")\n",
    "\n",
    "most_id_for_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将MIFF合并回df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.combine_first(df[\"_file_path\"].to_frame().join(most_id_for_file, on=\"_file_path\"))\n",
    "df[\"_prime_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. 验证\n",
    "目前，只有QQ账单反馈的数据没有主体ID，使用文件内最多ID作为这类文件的主体已经足够，  \n",
    "所有prime_id都应该得到补充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"_prime_id\"].isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，每一个独特的prime_id都应该对应一个prime_name，不会对应两个人，可以验证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = df[[\"_prime_id\", \"prime_name\"]].drop_duplicates(subset=[\"_prime_id\"])\n",
    "assert all_ids.drop_duplicates().__len__() == all_ids.__len__()\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. 补全缺失主体名字\n",
    "对之前仍然不能确定的主体名字，可尝试用prime_id在其他文件下关联的name来确定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. 同名人员分离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"prime_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 99. debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
