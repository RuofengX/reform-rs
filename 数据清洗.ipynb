{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 依赖声明\n",
    "\n",
    "运行以下代码安装依赖\n",
    "\n",
    "```shell\n",
    "conda install ipython ipykernel ipywidgets pyarrow pandas -y\n",
    "pip install hanlp[full] cachier\n",
    "```\n",
    "\n",
    "必须用conda安装兼容的pyarrow和pandas的环境依赖  \n",
    "\n",
    "然后pip安装hanlp以及持久缓存  \n",
    "\n",
    "安装nbstrpout工具，在每次提交时删除此文件中的敏感数据\n",
    "```\n",
    "pip install nbstripout\n",
    "nbstripout --install\n",
    "nbstripout --status\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import Optional, Any\n",
    "from pprint import pprint as print\n",
    "\n",
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "import hanlp\n",
    "from hanlp.components.mtl.tasks.ner.tag_ner import TaggingNamedEntityRecognition\n",
    "from hanlp.components.tokenizers.transformer import TransformerTaggingTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from cachier import cachier\n",
    "\n",
    "cachier = cachier(cache_dir='.py_cache')\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1. NLP配置\n",
    " ~~大炮打蚊子的第一步是把大炮架起来~~\n",
    "\n",
    "这里使用HanLP作为辅助处理工具，文档可以参考：[Github文档](https://github.com/hankcs/HanLP/tree/doc-zh)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载世界最大中文语料库\n",
    "_nlp_model = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH)  # type: ignore\n",
    "# ner: TaggingNamedEntityRecognition = _nlp_model['ner/msra']\n",
    "# ner.dict_whitelist = {'全账户': \"\"}\n",
    "\n",
    "# 配置精分固定短语\n",
    "_nlp_model['tok/fine'].dict_force = {\"全账户\"}\n",
    "\n",
    "\n",
    "# ner的意思是`命名实体识别`，可有效识别出人名、组织名等\n",
    "@cachier\n",
    "def ner(value) -> list[tuple[str, str, int, int]]:\n",
    "    result = _nlp_model(\n",
    "        value,\n",
    "        tasks=\"ner/msra\",\n",
    "    )[\"ner/msra\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "def ner_first_name(value: str) -> Optional[str]:\n",
    "    for content, type, _start, _end in ner(value):\n",
    "        if type == \"PERSON\":\n",
    "            return content\n",
    "\n",
    "\n",
    "def ner_path_rev(path: str) -> Optional[str]:\n",
    "    path = str(path)\n",
    "    # 将value按照分割符号切割，避免错误识别\n",
    "    path_part = path.split(os.path.sep)\n",
    "\n",
    "    # 从后往前迭代，更后的文件夹更有可能是文件的名字\n",
    "    path_part.reverse()\n",
    "    for i in path_part:\n",
    "        result = ner_first_name(i)\n",
    "        if result is None:\n",
    "            pass\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = _nlp_model(\n",
    "    [\"全账户分析自动化脚本，作者韦若枫，mailto://i@ruofengx.cn\"],\n",
    "    tasks=\"ner/msra\",\n",
    ")\n",
    "\n",
    "result.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 读取数据\n",
    "\n",
    "读取由rust程序生成的parquet文件\n",
    "\n",
    "**数据列说明**\n",
    "\n",
    "- **_config_name**  \n",
    "  \n",
    "  `str`\n",
    "  \n",
    "  前期处理的配置名称\n",
    "\n",
    "- **_datetime**  \n",
    "  \n",
    "  `datetime.datetime`\n",
    "  \n",
    "  转账的日期，实际精度到秒。\n",
    "  \n",
    "  存在部分银行整点执行的自动划转，实际精度为日，精度自动扩大至当日零时零分零秒。  \n",
    "\n",
    "- **_amount**  \n",
    "\n",
    "  `decimal.Decimal`\n",
    "  \n",
    "  保留两位小数，整数部分精度为36位\n",
    "\n",
    "- **_from_id**  \n",
    "\n",
    "  `str`\n",
    "  \n",
    "  转账发出人在该平台的唯一ID\n",
    "\n",
    "  应为非空字符串，但实际上存在空字符串的可能，需进一步研究TODO\n",
    "\n",
    "- **_from_bank_id**  \n",
    "\n",
    "  `Optional[str]`\n",
    "  \n",
    "  转账发出人的银行卡号，如账单未定义则为None\n",
    "\n",
    "  存在部分平台账单在此列数据填入非银行卡号，需要正则清洗\n",
    "  为非空字符串，但实际上存在空字符串的可能，建议drop\n",
    "\n",
    "  也有的银行账单会填入存折号，清洗时注意\n",
    "\n",
    "- **_from_name**  \n",
    "\n",
    "  `Optional[str]`\n",
    "  \n",
    "  转账发出人的名字，也可以是商家名字\n",
    "\n",
    "- **_to_id**\n",
    "\n",
    "  `str`\n",
    "  \n",
    "  转账接收在该平台的唯一ID\n",
    "\n",
    "  应为非空字符串，但实际上存在空字符串的可能，需进一步研究TODO\n",
    "\n",
    "- **_to_bank_id**\n",
    "\n",
    "  `Optional[str]`类型，转账接收人的银行卡号，如账单未定义则为None\n",
    "\n",
    "  存在部分平台账单在此列数据填入非银行卡号，需要正则清洗\n",
    "  为非空字符串，但实际上存在空字符串的可能，建议drop\n",
    "\n",
    "  也有的银行账单会填入存折号，清洗时注意\n",
    "\n",
    "\n",
    "- **_to_name**\n",
    "\n",
    "  `Optional[str]`类型，转账发出人的名字，也可以是商家名字\n",
    "\n",
    "- **_file_path**\n",
    "\n",
    "  `str`\n",
    "\n",
    "  文件路径，可通过NLP分析出账单主体，并和**_from_name**或**_to_name**对比，确认查询主体\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"./tmp/batch.parquet\")\n",
    "print(df.columns.tolist())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. 检查数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. 检查双侧ID均空的行\n",
    "\n",
    "输出结果应为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['_from_id'].isnull() & df['_to_id'].isnull()].__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 定义数据模型(暂时无用)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目前没啥用\n",
    "@dataclass\n",
    "class Entity:\n",
    "    id: str\n",
    "    bank_id: Optional[str]\n",
    "    name: Optional[str]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Record:\n",
    "    config: str\n",
    "    time: datetime\n",
    "    amount: Decimal\n",
    "    from_: Entity\n",
    "    to: Entity\n",
    "    path: str\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return (\n",
    "            self.time.__hash__()\n",
    "            + self.amount.__hash__()\n",
    "            + self.from_.id.__hash__()\n",
    "            + self.to.id.__hash__()\n",
    "        )\n",
    "\n",
    "\n",
    "def row_to_record(row: pd.Series) -> Record:\n",
    "    from_entity = Entity(\n",
    "        id=row[\"_from_id\"], bank_id=row[\"_from_bank_id\"], name=row[\"_from_name\"]  # type: ignore\n",
    "    )\n",
    "    to_entity = Entity(\n",
    "        id=row[\"_to_id\"], bank_id=row[\"_to_bank_id\"], name=row[\"_to_name\"]  # type: ignore\n",
    "    )\n",
    "    return Record(\n",
    "        config=row[\"_config_name\"],  # type: ignore\n",
    "        time=row[\"_datetime\"],  # type: ignore\n",
    "        amount=row[\"_amount\"],  # type: ignore\n",
    "        from_=from_entity,\n",
    "        to=to_entity,\n",
    "        path=row[\"_file_path\"],  # type: ignore\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算数据模型，并存储到`ent`列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"ent\"] = df.progress_apply(row_to_record, axis=1)  # type:ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 数据清洗\n",
    "\n",
    "1. 将该文件查询对象（查询的银行卡、财付通账号等等，总是_from_id或_to_id中的一个）关联至该主体，对于一个文件，查询对象总是至少占一半。这个规律不一定适用步骤2。\n",
    "2. 确定一个文件的主体姓名，首先通过NLP分析文件路径_file_path确定人名，其次可以通过出现频率大于一半的 _*_name 字段值即为主体人名\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 确定文件主体\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. 同文件级别最常出现的name\n",
    "\n",
    "`file_common_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1.1. 获取全部names\n",
    "\n",
    "的from_names和to_names，并合并为names表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_names = df[[\"_file_path\", \"_from_name\"]].copy()\n",
    "to_names = df[[\"_file_path\", \"_to_name\"]].copy()\n",
    "\n",
    "from_names.rename(columns={\"_from_name\": \"file_common_name\"}, inplace=True)\n",
    "to_names.rename(columns={\"_to_name\": \"file_common_name\"}, inplace=True)\n",
    "\n",
    "names = pd.concat([from_names, to_names])\n",
    "names.set_index(\"_file_path\", inplace=True)\n",
    "\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1.2. 统计name频数\n",
    "\n",
    "同文件的name出现频数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_names_count = names.groupby([\"_file_path\", \"file_common_name\"]).value_counts().reset_index()\n",
    "_names_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1.3. 递归过滤最常出现的name\n",
    "\n",
    "迭代file_common_name，利用ner函数排除机构组织名字，直至频数最大的file_common_name的为人名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 递归验证频率最高的name是人名，不是人名则从names_count删除，继续下一个\n",
    "# 这么做可以避免全量计算ner\n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    most_name_for_file = _names_count[[\"_file_path\", \"file_common_name\"]].loc[\n",
    "        _names_count.groupby(\"_file_path\")[\"count\"].idxmax()\n",
    "    ]\n",
    "    most_name_for_file[\"ner_name\"] = most_name_for_file[\n",
    "        \"file_common_name\"\n",
    "    ].progress_apply(ner_first_name)\n",
    "    bad_names = most_name_for_file[most_name_for_file[\"ner_name\"].isnull()][\n",
    "        \"file_common_name\"\n",
    "    ].to_list()\n",
    "    if bad_names.__len__() > 0:\n",
    "        _names_count = _names_count[\n",
    "            _names_count[\"file_common_name\"].apply(lambda x: x not in bad_names)\n",
    "        ]\n",
    "        print(f\"第{i}次迭代{_names_count.__len__()}\")\n",
    "        print(_names_count.shape)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1.4. 附加到df表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_name_for_file = most_name_for_file.set_index('_file_path').drop(columns=\"ner_name\") # 格式化表\n",
    "df = df.join(most_name_for_file, on=\"_file_path\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. 同文件夹最常出现的name\n",
    "\n",
    "`folder_common_name`\n",
    "\n",
    "file_common_name 会有部分为空，这种情况会出现在账单非常稀缺的情况，甚至干脆是空表。  \n",
    "这个时候可以用同样的办法研判同文件夹下最常出现的name作为依据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似file_common_name的步骤，不再赘述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取文件夹路径\n",
    "df[\"folder_path\"] = df[\"_file_path\"].apply(lambda x: os.path.dirname(x))\n",
    "# 取文件夹出现的名字表names\n",
    "from_names = df[[\"folder_path\", \"_from_name\"]].copy()\n",
    "to_names = df[[\"folder_path\", \"_to_name\"]].copy()\n",
    "from_names.rename(columns={\"_from_name\": \"folder_common_name\"}, inplace=True)\n",
    "to_names.rename(columns={\"_to_name\": \"folder_common_name\"}, inplace=True)\n",
    "names = pd.concat([from_names, to_names])\n",
    "names.set_index(\"folder_path\", inplace=True)\n",
    "\n",
    "\n",
    "# 对names计数得到most_name_for_folder\n",
    "_names_count = names.groupby([\"folder_path\", \"folder_common_name\"]).value_counts().reset_index()\n",
    "most_name_for_folder = _names_count[[\"folder_path\", \"folder_common_name\"]].loc[_names_count.groupby(\"folder_path\")[\"count\"].idxmax()]\n",
    "most_name_for_folder.set_index(\"folder_path\", inplace=True)\n",
    "\n",
    "# 迭代至全是人名\n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    most_name_for_folder = _names_count[[\"folder_path\", \"folder_common_name\"]].loc[\n",
    "        _names_count.groupby(\"folder_path\")[\"count\"].idxmax()\n",
    "    ]\n",
    "    most_name_for_folder[\"ner_name\"] = most_name_for_folder[\n",
    "        \"folder_common_name\"\n",
    "    ].progress_apply(ner_first_name)\n",
    "    bad_names = most_name_for_folder[most_name_for_folder[\"ner_name\"].isnull()][\n",
    "        \"folder_common_name\"\n",
    "    ].to_list()\n",
    "    if bad_names.__len__() > 0:\n",
    "        _names_count = _names_count[\n",
    "            _names_count[\"folder_common_name\"].apply(lambda x: x not in bad_names)\n",
    "        ]\n",
    "        print(f\"第{i}次迭代{_names_count.__len__()}\")\n",
    "        print(_names_count.shape)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 并入df\n",
    "most_name_for_folder = most_name_for_folder.set_index('folder_path').drop(columns=\"ner_name\") # 格式化表\n",
    "df = df.join(most_name_for_folder, on=\"folder_path\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. 从路径推理name\n",
    "\n",
    "`path_reason_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3.1. 举个例子\n",
    "\n",
    "可能不同的数据集的例子不一样，所以看看就行。  \n",
    "数据已作脱敏处理。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "example = df.iloc[277572]\n",
    "example\n",
    "```\n",
    "\n",
    "输出:\n",
    "```txt\n",
    "_config_name                                                    国反-三方\n",
    "_datetime                                         2022-03-10 17:41:32\n",
    "_amount                                                        193.99\n",
    "_from_id                                            134****055@qq.com\n",
    "_from_bank_id                                                    None\n",
    "_from_name                                                       None\n",
    "_to_id                                                             花呗\n",
    "_to_bank_id                                                      None\n",
    "_to_name                                                         None\n",
    "_file_path          ./tmp/batch/倪*全账户/134****055@qq.com(2021010100...\n",
    "...\n",
    "Name: 277572, dtype: object\n",
    "```\n",
    "\n",
    "这一行没有任何当事人的信息，只有一个_from_id，还是QQ邮箱地址，只有path中包含主体名字“倪*”。  \n",
    "如果此时`file_common_name`和`folder_common_name`均无效的话，就需要通过NLP对文件路径进行提取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用函数`ner_path`\n",
    "```python\n",
    "ner_path(example[\"_file_path\"])\n",
    "```\n",
    "得到想要的输出：\n",
    "\n",
    "'倪*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3.2. 正式开始\n",
    "\n",
    "首先，查看有多少文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"_file_path\"].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为涉及神经网络计算，速度会很慢，建议以下几点：\n",
    "* 对开头定义的ner加上持久化缓存cachier，对同一个路径碎片可以直接调用缓存\n",
    "* 安装`HanLP[full]`配套CUDA食用（未测试），或使用强力CPU（7950X3D大约50it/s）\n",
    "* 使用本地jupyter环境，远程环境带宽可以是瓶颈\n",
    "* 先去重，再计算，随后使用`df.join`方法插回去\n",
    "* 添加tqdm进度条，需要安装依赖，可参考[ipywidgets文档](https://ipywidgets.readthedocs.io/en/stable/user_install.html)，在开头应该安装过了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_reason_names = df.drop_duplicates(\"_file_path\")[[\"_file_path\"]]\n",
    "path_reason_names[\"path_reason_name\"]= path_reason_names[\"_file_path\"].progress_apply(ner_path_rev)\n",
    "path_reason_names.set_index(\"_file_path\", inplace=True)\n",
    "\n",
    "# 合并到df\n",
    "df = df.join(path_reason_names, on=\"_file_path\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. 合并主体\n",
    "\n",
    "`main_name`\n",
    "\n",
    "目前有三个不同的主体类型，需要推断、清洗，得出最后的类型\n",
    "\n",
    "一般来说，`file_common_name`作为文件中最常出现的名字，有较高可信度。  \n",
    "但是部分账单这条不适用：\n",
    "\n",
    "比如（数据已脱敏）：\n",
    "```txt\n",
    "_config_name                                                      国反-三方\n",
    "_datetime                                           2024-03-25 15:51:58\n",
    "_amount                                                            6.00\n",
    "_from_id                                                    158******49\n",
    "_from_bank_id                                                      None\n",
    "_from_name                                                         None\n",
    "_to_id                                                t*******3@t**u.cn\n",
    "_to_bank_id                                                        None\n",
    "_to_name                                                   厦门*******有限公司\n",
    "_file_path            ./tmp/batch/8人全账户流水/刘*/158******49(20240101000...\n",
    "file_common_name                                                    NaN\n",
    "_folder_path                                     ./tmp/batch/8人全账户流水/刘*\n",
    "folder_common_name                                                   刘*\n",
    "path_reason_name                                                     刘*\n",
    "Name: 16554, dtype: object\n",
    "```\n",
    "这条账单中，`file_common_name`指向了一个公司，这里就需要将这类例子进行排除，  \n",
    "并先后以`path_reason_name`和`folder_common_name`取代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先把\"(个人)\"从字符串中删除  \n",
    "这个无效标记在国反三方账单中非常常见\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OP_COL = [\"file_common_name\", \"folder_common_name\", \"path_reason_name\"]\n",
    "for col in OP_COL:\n",
    "    df[col] = df[col].str.replace(\"(个人)\", \"\", regex=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当`path_reason_name`为空时,此时说明文件夹制作者没有对文件进行分类，而是直接混杂放置  \n",
    "这个时候`folder_common_name`也不可信，直接选用`file_common_name`作为主体名字  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_row = df[df[\"path_reason_name\"].isnull()].index\n",
    "df.loc[filter_row, \"main_name\"] = df.loc[filter_row, \"file_common_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "路径已成功解析名字的可以直接作为主体名字  \n",
    "TODO: 可以额外手动验证下冲突项中是否有错误的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_row = df[df[\"path_reason_name\"].notnull()].index\n",
    "df.loc[filter_row, \"main_name\"] = df.loc[filter_row, \"path_reason_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还会剩余一些“三无”记录  \n",
    "只能通过ID进行关联"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未研判主体记录的数量\n",
    "df[df[\"main_name\"].isna()].__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 99. debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_conflict(row: pd.Series) -> bool:\n",
    "    a = row[\"file_common_name\"]\n",
    "    b = row[\"folder_common_name\"]\n",
    "    c = row[\"path_reason_name\"]\n",
    "    if a is None or b is None or c is None:\n",
    "        return True\n",
    "    return (a != b) | (a != c) | (b != c)\n",
    "\n",
    "def clean_data(df):\n",
    "    df = df[df.apply(is_conflict, axis=1)]\n",
    "    df = df[df[\"main_name\"].isnull()]\n",
    "    # 删除列: '_config_name'、'_datetime'和其他列8\n",
    "    # df = df.drop(columns=['_config_name', 'folder_path', '_datetime', '_amount', '_from_id', '_from_bank_id', '_from_name', '_to_id', '_to_bank_id', '_to_name', 'ent'])\n",
    "    # 删除列: '_file_path' 中的重复行\n",
    "    # df = df.drop_duplicates(['_file_path'])\n",
    "    return df\n",
    "\n",
    "df_clean = clean_data(df.copy())\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_row = df.loc[16554]\n",
    "debug_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_path_rev(debug_row[\"_file_path\"])  # type:ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
